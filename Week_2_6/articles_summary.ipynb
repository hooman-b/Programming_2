{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Engineering for Machine Learning: Characterizing and Detecting Mismatch in Machine-Learning Systems\n",
    "Grace Lewis and Ipek Ozkaya, May 17, 2021, <a href=\"https://insights.sei.cmu.edu/blog/software-engineering-for-machine-learning-characterizing-and-detecting-mismatch-in-machine-learning-systems/\" target=\"_blank\">link</a>\n",
    "### Introduction\n",
    "The passage discusses the growing interest in incorporating artificial intelligence (AI) and machine-learning (ML) components into software systems. This interest is fueled by the availability of tools for developing ML components and their potential to improve solutions for data-driven decision problems. However, integrating ML components into production systems presents challenges. Developing an ML system involves more than just building a model; it requires testing for production readiness, integration into larger systems, real-time monitoring, and adaptation to changing data. As a result of these complexities, a new field called software engineering for machine learning (SE4ML) is emerging.\n",
    "\n",
    "An ML-enabled system is a software system that uses ML components for its capabilities. To create effective ML-enabled systems:\n",
    "\n",
    "1. Integration of ML components should be easy.\n",
    "    \n",
    "2. The system must be set up for runtime monitoring of ML components and production data.\n",
    "    \n",
    "3. The training and retraining cycle of these systems should be fast.\n",
    "\n",
    "While some software engineering practices directly apply to these requirements, they are often not commonly used in the data science field, which focuses on developing ML algorithms/models for integration. Adapting or extending software engineering practices is necessary to effectively deal with ML components.\n",
    "\n",
    "### ML Mismatch\n",
    "\n",
    "The challenge of integrating ML components into applications is hindered by various factors, including discrepancies between different system components. This issue arises due to the involvement of three distinct disciplines in the development and deployment of ML-enabled systems: data science, software engineering, and operations. When these disciplines' perspectives don't align, it leads to ML mismatches, resulting in system failures.\n",
    "\n",
    "ML components differ from traditional software components because they heavily rely on data. Their performance in production depends on how closely the production data resembles the data used to train the ML model. This dependency is known as the \"training-serving skew.\" For ML-enabled systems to succeed, they need to offer a method to detect declining model performance and provide sufficient information for effective model retraining when this occurs.\n",
    "\n",
    "Here are some examples of mismatch in ML-enabled systems:\n",
    "\n",
    "1. **Insufficient Computing Resources**: Poor system performance arises when production environments lack the necessary computing resources to execute ML models effectively.\n",
    "\n",
    "2. **Data Compatibility Issues**: Model accuracy suffers due to disparities between training and production data, impacting the model's real-world performance.\n",
    "\n",
    "3. **API Alignment Problems**: Integrating ML components requires extensive glue code when the expected inputs and outputs differ from those provided by the system.\n",
    "\n",
    "4. **Testing Complexities**: Limited access to appropriate test data, incomplete understanding of components, and inadequate testing approaches hinder software engineers from thoroughly testing ML integration.\n",
    "\n",
    "5. **Monitoring Hurdles**: Production environment monitoring tools often fail to capture crucial ML metrics, such as model accuracy, complicating performance assessment.\n",
    "\n",
    "As part of our work on SE4ML, we developed a set of machine-readable descriptors for elements of ML-enabled systems that externalize and codify the assumptions made by different system stakeholders. The goal for the descriptors is to support automated detection of mismatches at both design time and run time.\n",
    "\n",
    "### Study to Characterize and Codify ML Mismatches\n",
    "To address the challenge of ML mismatches, the team embarked on a project focused on creating machine-readable descriptors for components within ML-enabled systems. These descriptors capture and formalize the various assumptions made by different stakeholders involved in the system. The aim of these descriptors is to facilitate automated identification of mismatches during both the design and runtime phases.\n",
    "\n",
    "Within the scope of this project, a two-phase study was conducted. In Phase 1, practitioners were interviewed to uncover instances of mismatches and their associated consequences. Simultaneously, a review of existing literature was conducted to identify documented best practices for developing ML systems. Moving to Phase 2, the validated mismatches identified through interviews were aligned with the system attributes outlined in the literature review. This alignment enabled the definition of attributes that would allow the detection of each specific mismatch. These attributes were then formalized into descriptors using the JSON schema.\n",
    "\n",
    "### Phase 1 Results\n",
    "In general they could detect seven categories of missmatches. The categories are as follows:\n",
    "\n",
    "1. **trained model**: Mismatches in this category split evenly between information related to test cases and test data, and lack of information about the API and specifications.\n",
    "\n",
    "2. **operational environment**: Many of the mismatches in this category were related to runtime metrics and data. The model was put into operation, and the operations staff did not know what they were supposed to monitor.\n",
    "\n",
    "3. **task and purpose**: This category pertains to requirements: It focuses on the necessary communication between project owners and data scientists to ensure that the model developed by data scientists aligns with the expectations of the project owners.\n",
    "\n",
    "4. **raw data**: Among the raw-data mismatches, many arose due to missing metadata. This includes essential details such as the data's collection methodology, time of collection, distribution approach, geographical origin, and the specific time frames during which it was gathered. Additionally, mismatches often involved incomplete descriptions of data elements, encompassing aspects like field names, explanations, values, and interpretations of any absent or null values.\n",
    "\n",
    "5. **development environment**:The majority of these mismatches were related to programming languages. Mismatches frequently arose due to data scientists neglecting to communicate the programming language employed in developing the model, or software engineers not conveying information about the programming language utilized within the system itself.\n",
    "\n",
    "6. **operational data**: Most mismatches here stemmed from lack of operational data statistics.\n",
    "\n",
    "7. **training data**:\n",
    "\n",
    "The majority of identified mismatches stem from inaccurate assumptions regarding the trained model (36%), which is developed by data scientists and integrated by software engineers into larger systems. The second most prevalent category is the operational environment (16%), encompassing the computing setting where the trained model operates, known as the model-serving or production environment.\n",
    "\n",
    "Subsequent categories include task and purpose (15%), defining the model's expected functions and limitations, and raw data (10%), representing the data on which the training data is based. In smaller proportions, mismatches relate to the development environment (9%), utilized by software engineers for integrating and testing the model; operational data (8%), the data processed by the model during its operation; and training data (6%), employed to train the model. To find some of the examples of the different missmatch categories one can delve into following <a href=\"https://insights.sei.cmu.edu/blog/software-engineering-for-machine-learning-characterizing-and-detecting-mismatch-in-machine-learning-systems/\" target=\"_blank\">link</a>.\n",
    "\n",
    "### Coclusion\n",
    "The findings from Phase 1 of the study indicate that enhancing communication and automating the awareness and detection of ML mismatches can contribute to the enhancement of software engineering for ML-enabled systems.\n",
    "\n",
    "### Peronal Examples:\n",
    "I lack any industry experience in my background. In fact, my entire career has been within academic settings. As a result, I will attempt to align some of my academic experiences with the mentioned criteria and highlight the most relevant connections here. It's important to note that the specified mismatches can also arise within an academic environment, particularly among research group members collaborating on shared projects.\n",
    "\n",
    "my experiences are:\n",
    "\n",
    "**trained model**: \n",
    "\n",
    "Regarding this category, I'm currently facing a mismatch while working on my internship project. My task involves creating or improving a deep learning model to predict side effects of head and neck cancer. My supervisor suggested I begin by using a previously trained model, which was developed for similar tasks, on my dataset. The idea was to later improve upon it to achieve higher optimization and accuracy with my own model. \n",
    "\n",
    "However, as I started working with their model, I encountered an issue. I couldn't locate any documentation regarding the trained model, such as a readme file or additional explanations about its functioning. This lack of information made it challenging for me to comprehend how the model operates and integrate it effectively. Consequently, I reached out to my supervisor and asked for an explanation of the model's workings.\n",
    "\n",
    "**operational environment**:\n",
    "\n",
    "In terms of this mismatch category, During my participation in the 'Integrated Omics Project' course, I encountered a noteworthy experience. In this project, I constructed a pipeline responsible for processing raw images obtained from a biological organism by using x-ray microscope. This pipeline aimed to transform these images into a collection of endmembers, which could later be further refined into colored images using techniques like the FCLS method. However, it's important to note that these transformation techniques were time-consuming. Therefore, it held great significance to assess the quality of the image transformations beforehand, utilizing evaluation metrics.\n",
    "\n",
    "With this goal in mind, I engaged with a colleague to make adjustments to the preliminary conditions of the image preprocessing stage within the pipeline. Regrettably, I omitted to explicitly communicate the requirement of employing evaluation metrics to gauge the resulting image quality. Consequently, my colleague proceeded under the assumption that running the pipeline and obtaining results were the primary objectives.\n",
    "\n",
    "Upon gathering the results, we proceeded to employ the FCLS method to generate colored images from the endmembers. However, after consuming a week to execute across all endmembers groups, when we finally examined the resulting images, it became apparent that the endmember groups did not have the required quality. I recognized that I had forgotten to mention the essential use of evaluation metrics within the pipeline to my colleague, which could have solved this issue.\n",
    "\n",
    "**task and purpose**:\n",
    "\n",
    "Again I experienced 'task and purpose' mismatch during 'Integrated Omics Project' course. In the initial months, our group was left somewhat uncertain about our supervisor's precise expectations. Our task was to implement various methods for extracting abundance maps using different techniques. However, the ultimate goal and desired outcome of the project remained unclear, leaving us without a definitive endpoint or conclusion in mind.\n",
    "\n",
    "Asking our supervisor for clarification about the project's intended endpoint shed light on the matter. It turned out that our objective was to categorize all the abundance maps and create distinct groups based on similarities in energy channels. This categorization was intended to aid biologists in effectively comparing the abundance maps and identifying the most accurate and relevant set.\n",
    "\n",
    "**raw data**:\n",
    "\n",
    "I encountered this issue while delving into the Titanic dataset for one of the exercises in my educational machine learning notebook. While others had previously explored this dataset and shared its metadata on their websites, I chose to rely solely on the Kaggle metadata provided for the dataset, which was unfortunately incomplete. My decision to do so was deliberate, as I aimed to demonstrate that a substantial amount of information about the dataset could be uncovered using the dataset itself.\n",
    "\n",
    "Although I managed to successfully deduce the meaning of each feature, understand the measurement scales for these features, and decipher the implications of null values, this process consumed a considerable 2 to 3 days of my time. This experience underscored the significance of metadata for a data scientist—it can greatly streamline the research process and significantly expedite understanding.\n",
    "\n",
    "**development environment**:\n",
    "\n",
    "I've never encountered a mismatch arising from the use of different programming languages in a project. However, I did come across a situation during the 'Integrated Omics Project' course that falls into a similar mismatch category. In this particular course, my colleague and I each developed our own individual pipelines for the project. Yet, when we compared the results of these two pipelines, we noticed a discrepancy in the outcomes. The abundance maps extracted from the two pipelines using the same methods turned out to be different.\n",
    "\n",
    "To address this, we spent a whole day investigating the issue. Eventually, we identified the source of the difference: the implementation of the PCA (Principal Component Analysis) method. My colleague employed the sklearn PCA package, while I opted to build the PCA from scratch. Upon reviewing the classes within the package, we discovered that the sklearn PCA package was based on the SVD (Singular Value Decomposition) method. In contrast, my approach involved matrix decomposition into eigenvalues and eigenvectors. This discrepancy in implementation led to the variation in our results.\n",
    "\n",
    "**operational data**:\n",
    "\n",
    "I have never experienced this one since all the test data that I used in my course are a part of my main dataset, and I randomly sliced a part of that for testing porpuses. Moreover, I have just started my internship, and I do not design my model yet let alone implementing it in a real situation. Furthermore, during 'Integrated Omics Project' course, the test data given to use had exactly the same shape and characteristics of the training data.\n",
    "\n",
    "**training data**:\n",
    "\n",
    "I have never had an experience like this. One reason can be I do not have much experience in this field, and another reason is that we have been tought to seperate the preprocessing part from the model designing section. Consequently, we always consider a seperate distinct part for preparing our training dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackling Collaboration Challenges in the Development of ML-Enabled Systems \n",
    "Grace Lewis, February 27, 2023, <a href=\"https://insights.sei.cmu.edu/blog/tackling-collaboration-challenges-in-the-development-of-ml-enabled-systems/\" target=\"_blank\">link</a>.\n",
    "\n",
    "### Introduction\n",
    "In software projects involving multiple developers, collaboration is essential. Development tasks are divided into system components, with team members working independently until integration. Component interfaces play a key role in determining collaboration points. Challenges arise when communication is difficult or interdisciplinary collaboration is needed. Experience, backgrounds, and differing expectations can hinder traditional development projects. Strategies and informal tools aid collaboration. Software lifecycle models like waterfall, spiral, and Agile assist in planning interfaces.\n",
    "\n",
    "ML-enabled systems combine traditional development with ML components. This requires coordination between data science and software engineering for model creation, interface negotiation, and system operation. Data science expertise is vital for effective model building, but software engineers attempting this without proper training often create ineffective models. Data scientists may overlook engineering aspects affecting their models. Recent focus in software engineering research has been on testing, deployment, fairness, and robustness of ML models, with limited exploration of system-wide perspectives for ML-enabled systems.\n",
    "\n",
    "### The structure of the research\n",
    "Given the scarcity of existing research on collaboration within ML-enabled system development, they employed a qualitative approach for their study, organized into four key phases: (1) defining scope and conducting a comprehensive literature review, (2) engaging in interviews with professionals engaged in ML-enabled system construction, (3) corroborating the interview insights with findings from the literature review, and (4) validating the conclusions through feedback from the interviewees.\n",
    "\n",
    "During their analysis, they found that certain teams were tasked with both model and software development, while in other scenarios, distinct teams managed software and model development separately. Across all the teams they studied, they did not identify any overarching trends that applied universally. However, as they shifted their focus to specific collaboration aspects, patterns did become apparent:\n",
    "\n",
    "1. Requirements and Planning\n",
    "2. Training Data\n",
    "3. Product-Model Integration\n",
    "\n",
    "Within these specific areas, they observed consistent trends and dynamics emerging among the teams they examined.\n",
    "\n",
    "### Requirements and planning\n",
    "There are different orders with which teams identify product and model requirements:\n",
    "\n",
    "1. **Model first**: These teams build the model first and then build the product around the model. The model shapes product requirements\n",
    "\n",
    "2. **Product first**: hese teams start with product development and then develop a model to support it. Most often, the product already exists, and new ML development seeks to enhance the product’s capabilities. in this group product requirements shape the model requirements.\n",
    "\n",
    "3. **Parallel**: The model and product teams work in parallel.\n",
    "\n",
    "Regardless of the type of trajectory a company is working with, they may face some tensions between product and model requirements.\n",
    "\n",
    "1. Product requirements require input from the model team.\n",
    "\n",
    "2. Model development with unclear requirements is common.\n",
    "\n",
    "3. Provided model requirements rarely go beyond accuracy and data security. \n",
    "\n",
    "To tackle these tension, the authors present four solutions:\n",
    "\n",
    "1. Involve data scientists early in the process.\n",
    "   \n",
    "2. Consider adopting a parallel development trajectory for product and model teams.\n",
    "   \n",
    "3. Conduct ML training sessions to educate clients and product teams.\n",
    "    \n",
    "4. Adopt more formal requirements documentation for both model and product.\n",
    "\n",
    "### Training Data\n",
    "This research unveiled that disputes regarding training data stood out as the predominant collaboration hurdles. These conflicts frequently arise due to the model team not being responsible for data ownership, collection, or comprehension. Their observations identified three distinct organizational structures that contribute to the collaboration challenges linked with training data:\n",
    "\n",
    "1. **Provided data**: the data is provided to the model team by the product team.\n",
    "\n",
    "2. **External data**: the model team gathers data from public resources or from a third-party.\n",
    "\n",
    "3. **In-house data**: the data belongs to their own company, so all the product, model, and data teams can use this in-house data.\n",
    "\n",
    "A substantial number of interviewees expressed discontent with the quantity and quality of data. A frequent issue is that the product team often lacks a clear understanding of the requisite data quality and quantity. Additionally, the organizations they studied faced a range of other prevalent data-related challenges, including:\n",
    "\n",
    "1. Provided and public data are often inadequate.\n",
    "\n",
    "2. Data understanding and access to data experts often present bottlenecks.\n",
    "\n",
    "3. Ambiguity arises when hiring a data firm.\n",
    "\n",
    "4. There is a need to handle evolving data.\n",
    "\n",
    "5. In-house priorities and security concerns often obstruct data access.\n",
    "\n",
    "They emphesized on the point that Training data of sufficient quality and quantity is crucial for developing ML-enabled systems. Moreover, they proposed the following recommendations:\n",
    "\n",
    "1. Make sure to include funds in your budget for data collection and securing access to domain experts, or even consider forming a dedicated data team.\n",
    "\n",
    "2. Establish a formal contract that clearly outlines the expected standards for data quality and quantity.\n",
    "\n",
    "3. If you're collaborating with a dedicated data team, ensure that everyone has a clear understanding of the expectations.\n",
    "\n",
    "4. Think about implementing a data validation and monitoring system early on in the project to ensure data quality and consistency.\n",
    "\n",
    "### Product-Model Integration\n",
    "During this stage of collaboration, data scientists and software engineers need to collaborate closely, often spanning multiple teams. Unfortunately, conflicts frequently arise at this point due to unclear processes and responsibilities. Tensions can also emerge from varying practices and expectations. The challenges encountered during this collaboration phase typically fall into two main categories: clashes in team cultures stemming from differing responsibilities and quality assurance concerns related to both models and projects.\n",
    "\n",
    "#### Cultural clashes\n",
    "They noted several conflicts arising from distinctions between software engineering and data science cultures. These conflicts were exacerbated by a lack of clear delineation of responsibilities and boundaries:\n",
    "\n",
    "1. Team responsibilities often do not match capabilities and preferences.\n",
    "\n",
    "2. Siloing data scientists fosters integration problems.\n",
    "\n",
    "3. Technical jargon challenges communication. \n",
    "\n",
    "4. Code quality, documentation, and versioning expectations differ widely.\n",
    "\n",
    "#### Quality Assurance for Model and Product\n",
    "During development and integration, questions of responsibility for quality assurance often arise. they noted the following challenges:\n",
    "\n",
    "1. Goals for model adequacy are hard to establish.\n",
    "\n",
    "2. Confidence is limited without transparent model evaluation.\n",
    "\n",
    "3. Responsibility for system testing is unclear.\n",
    "\n",
    "4. Planning for online testing and monitoring is rare. \n",
    "\n",
    "Also, they recommend the following approaches to tackle these problems:\n",
    "\n",
    "1. Make quality assurance testing a priority and incorporate it into your planning.\n",
    "\n",
    "2. While the product team should oversee overall quality and system testing, it's crucial to involve the model team in establishing a monitoring and experimentation framework.\n",
    "\n",
    "3. Ensure you allocate resources, plan ahead, and facilitate structured feedback from the product engineering team to the model team.\n",
    "\n",
    "4. Promote the advantages of carrying out testing in a real production environment.\n",
    "\n",
    "5. Set precise quality criteria for both the model and the final product.\n",
    "\n",
    "### Conclusion\n",
    "In the conclusion part of this article beside the mentioned points in the article, they added four broad areas for improving collaboration in the development of ML-enabled systems:\n",
    "\n",
    "1. Effective Communication: To address issues stemming from miscommunication, they recommend fostering ML literacy among software engineers and managers, as well as promoting software engineering literacy among data scientists.\n",
    "\n",
    "2. Documentation: While practices for documenting model requirements, data expectations, and ensuring model quality are still evolving, existing interface documentation could serve as a valuable foundation. However, any approach should employ a language comprehensible to all parties engaged in the development process.\n",
    "\n",
    "3. Engineering: Project managers need to ensure adequate engineering proficiency for both ML and non-ML components while also promoting a mindset focused on product and operations.\n",
    "\n",
    "4. Process: The experimental, trial-and-error nature of ML model development doesn't inherently align with the structured approach of the traditional software development lifecycle. We recommend delving into additional research on integrated process lifecycles tailored to ML-enabled systems.\n",
    "\n",
    "## Questions\n",
    "\n",
    "A) How do you envision your cooperation as (future) data scientist with software engineers?\n",
    "\n",
    "As a future data scientist, I see collaboration with software engineers as a crucial aspect of delivering effective and efficient solutions. I recognize that data science and software engineering are interconnected disciplines, and our cooperation will contribute to the success of projects. To foster effective collaboration:\n",
    "\n",
    "1. **Clear Communication**: I will prioritize transparent and consistent communication with software engineers. This involves discussing project goals, data requirements, model specifications, and any potential challenges that may arise.\n",
    "\n",
    "2. **Shared Understanding**: I will work to bridge the gap between data science and software engineering by enhancing my understanding of software development principles. This shared knowledge will facilitate smoother integration of ML components into larger systems.\n",
    "\n",
    "3. **Multidisciplinary Teams**: Collaborating in Multidisciplinary teams will enable us to combine our expertise for more holistic and comprehensive solutions. Understanding each other's roles will lead to better decision-making and well-informed project planning.\n",
    "\n",
    "4. **Continuous Learning**: I will continuously learn about software engineering practices, coding standards, and architectural patterns. This knowledge will empower me to design data-driven solutions that align with scalable and maintainable software practices.\n",
    "\n",
    "B) In what way will you make your code scalable, readable and maintainable?\n",
    "\n",
    "In order to ensure that my code is scalable, readable, and maintainable, I will follow best practices that prioritize quality and collaboration::\n",
    "\n",
    "1. **Modular Design**: I will structure my code into modular components that serve specific purposes. This modular approach enhances readability and allows for easier maintenance and future updates.\n",
    "\n",
    "2. **Leverage object-oriented programming**: I will adhere to object-oriented principles and SOLID principles that can enhance the readability and maintainability of my code.\n",
    "\n",
    "3. **Efficient Algorithms**: I will select algorithms and data structures with optimal time and space complexity to handle larger datasets efficiently, contributing to scalability.\n",
    "\n",
    "4. **Keep functions and methods short**: I will ensure that functions and methods are kept short and focused on a single task. If a function exceeds a certain number of lines, I will consider refactoring it into smaller, more manageable functions. This approach improves readability.\n",
    "\n",
    "5. **DRY(Don’t Repeat Yourself) Principle**: I will leverage the DRY principle to enhance code maintainability by avoiding code duplication. I will actively seek opportunities to extract common functionality into reusable functions or classes. This practice will reduce code redundancy and contribute to easier maintenance and updates in the codebase.\n",
    "\n",
    "6. **Clear Naming and Comments**: Meaningful variable and function names, along with informative comments, will make my code more understandable to both fellow data scientists and software engineers.\n",
    "\n",
    "7. **Version Control**: Utilizing version control systems like GitHub will help me track changes, collaborate effectively, and maintain a history of code modifications.\n",
    "\n",
    "8. **Testing and Validation**: I will implement thorough testing procedures to identify and fix bugs early in the development process. Automated testing will ensure the reliability of my code and help maintain its quality.\n",
    "\n",
    "9. **Documentation**: I will document my code comprehensively, explaining its purpose, functionality, and how to use it. This documentation will facilitate future modifications and collaborations.\n",
    "\n",
    "10. **Collaborative Code Reviews**: Engaging in code reviews with peers, including software engineers, will provide valuable feedback and insights for improving code quality and maintaining consistency.\n",
    "\n",
    "11. **YAGNI (You Aren't Gonna Need It)**: It means that if something is not necessary, do not implement it. Consequently, I will try to constrain myself to the core requirements.\n",
    "\n",
    "12. **KISS (Keep It Simple and Stupid)**: It states that designs and/or systems should be as simple as possible. Consequently, I will try to keep my solusions for a problem as simple as possible.\n",
    "\n",
    "By embracing these principles and practices, I aim to create code that not only fulfills immediate project goals but also contributes to the long-term success and maintainability of the systems I help build."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
