{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook\n",
    "In this notebook, I will test all the functions and classes that I am making in the main program to see the outcome of them dynamically.\n",
    "\n",
    "## Evaluate the DataDivider class\n",
    "in this part I will analyze the data and adjuct the DataDivider class for this assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the general libraries\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Isolation detection module\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# statistical module\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "# Feature Engineering module\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Model selection modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metric modules\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired by https://fennaf.gitbook.io/bfvm22prog1/data-processing/configuration-files/yaml\n",
    "\n",
    "def configReader():\n",
    "    \"\"\"\n",
    "    explanation: This function open config,yaml file \n",
    "    and fetch the gonfigue file information\n",
    "    input: ...\n",
    "    output: configue file\n",
    "    \"\"\"\n",
    "    with open(\"config.yaml\", \"r\") as inputFile:\n",
    "        config = yaml.safe_load(inputFile)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_43</th>\n",
       "      <th>sensor_44</th>\n",
       "      <th>sensor_45</th>\n",
       "      <th>sensor_46</th>\n",
       "      <th>sensor_47</th>\n",
       "      <th>sensor_48</th>\n",
       "      <th>sensor_49</th>\n",
       "      <th>sensor_50</th>\n",
       "      <th>sensor_51</th>\n",
       "      <th>machine_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-01 00:00:00</td>\n",
       "      <td>2.465394</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.310760</td>\n",
       "      <td>634.3750</td>\n",
       "      <td>76.45975</td>\n",
       "      <td>13.41146</td>\n",
       "      <td>16.13136</td>\n",
       "      <td>15.56713</td>\n",
       "      <td>...</td>\n",
       "      <td>41.92708</td>\n",
       "      <td>39.641200</td>\n",
       "      <td>65.68287</td>\n",
       "      <td>50.92593</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>157.9861</td>\n",
       "      <td>67.70834</td>\n",
       "      <td>243.0556</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-01 00:01:00</td>\n",
       "      <td>2.465394</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.310760</td>\n",
       "      <td>634.3750</td>\n",
       "      <td>76.45975</td>\n",
       "      <td>13.41146</td>\n",
       "      <td>16.13136</td>\n",
       "      <td>15.56713</td>\n",
       "      <td>...</td>\n",
       "      <td>41.92708</td>\n",
       "      <td>39.641200</td>\n",
       "      <td>65.68287</td>\n",
       "      <td>50.92593</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>157.9861</td>\n",
       "      <td>67.70834</td>\n",
       "      <td>243.0556</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-01 00:02:00</td>\n",
       "      <td>2.444734</td>\n",
       "      <td>47.35243</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.397570</td>\n",
       "      <td>638.8889</td>\n",
       "      <td>73.54598</td>\n",
       "      <td>13.32465</td>\n",
       "      <td>16.03733</td>\n",
       "      <td>15.61777</td>\n",
       "      <td>...</td>\n",
       "      <td>41.66666</td>\n",
       "      <td>39.351852</td>\n",
       "      <td>65.39352</td>\n",
       "      <td>51.21528</td>\n",
       "      <td>38.194443</td>\n",
       "      <td>155.9606</td>\n",
       "      <td>67.12963</td>\n",
       "      <td>241.3194</td>\n",
       "      <td>203.7037</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-01 00:03:00</td>\n",
       "      <td>2.460474</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.1684</td>\n",
       "      <td>46.397568</td>\n",
       "      <td>628.1250</td>\n",
       "      <td>76.98898</td>\n",
       "      <td>13.31742</td>\n",
       "      <td>16.24711</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>...</td>\n",
       "      <td>40.88541</td>\n",
       "      <td>39.062500</td>\n",
       "      <td>64.81481</td>\n",
       "      <td>51.21528</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>155.9606</td>\n",
       "      <td>66.84028</td>\n",
       "      <td>240.4514</td>\n",
       "      <td>203.1250</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-01 00:04:00</td>\n",
       "      <td>2.445718</td>\n",
       "      <td>47.13541</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.397568</td>\n",
       "      <td>636.4583</td>\n",
       "      <td>76.58897</td>\n",
       "      <td>13.35359</td>\n",
       "      <td>16.21094</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>...</td>\n",
       "      <td>41.40625</td>\n",
       "      <td>38.773150</td>\n",
       "      <td>65.10416</td>\n",
       "      <td>51.79398</td>\n",
       "      <td>38.773150</td>\n",
       "      <td>158.2755</td>\n",
       "      <td>66.55093</td>\n",
       "      <td>242.1875</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  sensor_00  sensor_01  sensor_02  sensor_03  sensor_04  \\\n",
       "0  2018-04-01 00:00:00   2.465394   47.09201    53.2118  46.310760   634.3750   \n",
       "1  2018-04-01 00:01:00   2.465394   47.09201    53.2118  46.310760   634.3750   \n",
       "2  2018-04-01 00:02:00   2.444734   47.35243    53.2118  46.397570   638.8889   \n",
       "3  2018-04-01 00:03:00   2.460474   47.09201    53.1684  46.397568   628.1250   \n",
       "4  2018-04-01 00:04:00   2.445718   47.13541    53.2118  46.397568   636.4583   \n",
       "\n",
       "   sensor_05  sensor_06  sensor_07  sensor_08  ...  sensor_43  sensor_44  \\\n",
       "0   76.45975   13.41146   16.13136   15.56713  ...   41.92708  39.641200   \n",
       "1   76.45975   13.41146   16.13136   15.56713  ...   41.92708  39.641200   \n",
       "2   73.54598   13.32465   16.03733   15.61777  ...   41.66666  39.351852   \n",
       "3   76.98898   13.31742   16.24711   15.69734  ...   40.88541  39.062500   \n",
       "4   76.58897   13.35359   16.21094   15.69734  ...   41.40625  38.773150   \n",
       "\n",
       "   sensor_45  sensor_46  sensor_47  sensor_48  sensor_49  sensor_50  \\\n",
       "0   65.68287   50.92593  38.194440   157.9861   67.70834   243.0556   \n",
       "1   65.68287   50.92593  38.194440   157.9861   67.70834   243.0556   \n",
       "2   65.39352   51.21528  38.194443   155.9606   67.12963   241.3194   \n",
       "3   64.81481   51.21528  38.194440   155.9606   66.84028   240.4514   \n",
       "4   65.10416   51.79398  38.773150   158.2755   66.55093   242.1875   \n",
       "\n",
       "   sensor_51  machine_status  \n",
       "0   201.3889          NORMAL  \n",
       "1   201.3889          NORMAL  \n",
       "2   203.7037          NORMAL  \n",
       "3   203.1250          NORMAL  \n",
       "4   201.3889          NORMAL  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframe_maker(config):\n",
    "    file_directory, file_name,_ = config.values()\n",
    "    os.chdir(file_directory)\n",
    "    df = pd.read_csv(file_name).drop('Unnamed: 0', axis=1)\n",
    "    return df\n",
    "df = dataframe_maker(configReader())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>sensor_09</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_43</th>\n",
       "      <th>sensor_44</th>\n",
       "      <th>sensor_45</th>\n",
       "      <th>sensor_46</th>\n",
       "      <th>sensor_47</th>\n",
       "      <th>sensor_48</th>\n",
       "      <th>sensor_49</th>\n",
       "      <th>sensor_50</th>\n",
       "      <th>sensor_51</th>\n",
       "      <th>machine_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:00:00</th>\n",
       "      <td>2.465394</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.310760</td>\n",
       "      <td>634.3750</td>\n",
       "      <td>76.45975</td>\n",
       "      <td>13.41146</td>\n",
       "      <td>16.13136</td>\n",
       "      <td>15.56713</td>\n",
       "      <td>15.05353</td>\n",
       "      <td>...</td>\n",
       "      <td>41.92708</td>\n",
       "      <td>39.641200</td>\n",
       "      <td>65.68287</td>\n",
       "      <td>50.92593</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>157.9861</td>\n",
       "      <td>67.70834</td>\n",
       "      <td>243.0556</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:01:00</th>\n",
       "      <td>2.465394</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.310760</td>\n",
       "      <td>634.3750</td>\n",
       "      <td>76.45975</td>\n",
       "      <td>13.41146</td>\n",
       "      <td>16.13136</td>\n",
       "      <td>15.56713</td>\n",
       "      <td>15.05353</td>\n",
       "      <td>...</td>\n",
       "      <td>41.92708</td>\n",
       "      <td>39.641200</td>\n",
       "      <td>65.68287</td>\n",
       "      <td>50.92593</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>157.9861</td>\n",
       "      <td>67.70834</td>\n",
       "      <td>243.0556</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:02:00</th>\n",
       "      <td>2.444734</td>\n",
       "      <td>47.35243</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.397570</td>\n",
       "      <td>638.8889</td>\n",
       "      <td>73.54598</td>\n",
       "      <td>13.32465</td>\n",
       "      <td>16.03733</td>\n",
       "      <td>15.61777</td>\n",
       "      <td>15.01013</td>\n",
       "      <td>...</td>\n",
       "      <td>41.66666</td>\n",
       "      <td>39.351852</td>\n",
       "      <td>65.39352</td>\n",
       "      <td>51.21528</td>\n",
       "      <td>38.194443</td>\n",
       "      <td>155.9606</td>\n",
       "      <td>67.12963</td>\n",
       "      <td>241.3194</td>\n",
       "      <td>203.7037</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:03:00</th>\n",
       "      <td>2.460474</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.1684</td>\n",
       "      <td>46.397568</td>\n",
       "      <td>628.1250</td>\n",
       "      <td>76.98898</td>\n",
       "      <td>13.31742</td>\n",
       "      <td>16.24711</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>15.08247</td>\n",
       "      <td>...</td>\n",
       "      <td>40.88541</td>\n",
       "      <td>39.062500</td>\n",
       "      <td>64.81481</td>\n",
       "      <td>51.21528</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>155.9606</td>\n",
       "      <td>66.84028</td>\n",
       "      <td>240.4514</td>\n",
       "      <td>203.1250</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:04:00</th>\n",
       "      <td>2.445718</td>\n",
       "      <td>47.13541</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.397568</td>\n",
       "      <td>636.4583</td>\n",
       "      <td>76.58897</td>\n",
       "      <td>13.35359</td>\n",
       "      <td>16.21094</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>15.08247</td>\n",
       "      <td>...</td>\n",
       "      <td>41.40625</td>\n",
       "      <td>38.773150</td>\n",
       "      <td>65.10416</td>\n",
       "      <td>51.79398</td>\n",
       "      <td>38.773150</td>\n",
       "      <td>158.2755</td>\n",
       "      <td>66.55093</td>\n",
       "      <td>242.1875</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sensor_00  sensor_01  sensor_02  sensor_03  sensor_04  \\\n",
       "timestamp                                                                    \n",
       "2018-04-01 00:00:00   2.465394   47.09201    53.2118  46.310760   634.3750   \n",
       "2018-04-01 00:01:00   2.465394   47.09201    53.2118  46.310760   634.3750   \n",
       "2018-04-01 00:02:00   2.444734   47.35243    53.2118  46.397570   638.8889   \n",
       "2018-04-01 00:03:00   2.460474   47.09201    53.1684  46.397568   628.1250   \n",
       "2018-04-01 00:04:00   2.445718   47.13541    53.2118  46.397568   636.4583   \n",
       "\n",
       "                     sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n",
       "timestamp                                                                    \n",
       "2018-04-01 00:00:00   76.45975   13.41146   16.13136   15.56713   15.05353   \n",
       "2018-04-01 00:01:00   76.45975   13.41146   16.13136   15.56713   15.05353   \n",
       "2018-04-01 00:02:00   73.54598   13.32465   16.03733   15.61777   15.01013   \n",
       "2018-04-01 00:03:00   76.98898   13.31742   16.24711   15.69734   15.08247   \n",
       "2018-04-01 00:04:00   76.58897   13.35359   16.21094   15.69734   15.08247   \n",
       "\n",
       "                     ...  sensor_43  sensor_44  sensor_45  sensor_46  \\\n",
       "timestamp            ...                                               \n",
       "2018-04-01 00:00:00  ...   41.92708  39.641200   65.68287   50.92593   \n",
       "2018-04-01 00:01:00  ...   41.92708  39.641200   65.68287   50.92593   \n",
       "2018-04-01 00:02:00  ...   41.66666  39.351852   65.39352   51.21528   \n",
       "2018-04-01 00:03:00  ...   40.88541  39.062500   64.81481   51.21528   \n",
       "2018-04-01 00:04:00  ...   41.40625  38.773150   65.10416   51.79398   \n",
       "\n",
       "                     sensor_47  sensor_48  sensor_49  sensor_50  sensor_51  \\\n",
       "timestamp                                                                    \n",
       "2018-04-01 00:00:00  38.194440   157.9861   67.70834   243.0556   201.3889   \n",
       "2018-04-01 00:01:00  38.194440   157.9861   67.70834   243.0556   201.3889   \n",
       "2018-04-01 00:02:00  38.194443   155.9606   67.12963   241.3194   203.7037   \n",
       "2018-04-01 00:03:00  38.194440   155.9606   66.84028   240.4514   203.1250   \n",
       "2018-04-01 00:04:00  38.773150   158.2755   66.55093   242.1875   201.3889   \n",
       "\n",
       "                     machine_status  \n",
       "timestamp                            \n",
       "2018-04-01 00:00:00          NORMAL  \n",
       "2018-04-01 00:01:00          NORMAL  \n",
       "2018-04-01 00:02:00          NORMAL  \n",
       "2018-04-01 00:03:00          NORMAL  \n",
       "2018-04-01 00:04:00          NORMAL  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('timestamp', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 220320 entries, 2018-04-01 00:00:00 to 2018-08-31 23:59:00\n",
      "Data columns (total 53 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   sensor_00       210112 non-null  float64\n",
      " 1   sensor_01       219951 non-null  float64\n",
      " 2   sensor_02       220301 non-null  float64\n",
      " 3   sensor_03       220301 non-null  float64\n",
      " 4   sensor_04       220301 non-null  float64\n",
      " 5   sensor_05       220301 non-null  float64\n",
      " 6   sensor_06       215522 non-null  float64\n",
      " 7   sensor_07       214869 non-null  float64\n",
      " 8   sensor_08       215213 non-null  float64\n",
      " 9   sensor_09       215725 non-null  float64\n",
      " 10  sensor_10       220301 non-null  float64\n",
      " 11  sensor_11       220301 non-null  float64\n",
      " 12  sensor_12       220301 non-null  float64\n",
      " 13  sensor_13       220301 non-null  float64\n",
      " 14  sensor_14       220299 non-null  float64\n",
      " 15  sensor_15       0 non-null       float64\n",
      " 16  sensor_16       220289 non-null  float64\n",
      " 17  sensor_17       220274 non-null  float64\n",
      " 18  sensor_18       220274 non-null  float64\n",
      " 19  sensor_19       220304 non-null  float64\n",
      " 20  sensor_20       220304 non-null  float64\n",
      " 21  sensor_21       220304 non-null  float64\n",
      " 22  sensor_22       220279 non-null  float64\n",
      " 23  sensor_23       220304 non-null  float64\n",
      " 24  sensor_24       220304 non-null  float64\n",
      " 25  sensor_25       220284 non-null  float64\n",
      " 26  sensor_26       220300 non-null  float64\n",
      " 27  sensor_27       220304 non-null  float64\n",
      " 28  sensor_28       220304 non-null  float64\n",
      " 29  sensor_29       220248 non-null  float64\n",
      " 30  sensor_30       220059 non-null  float64\n",
      " 31  sensor_31       220304 non-null  float64\n",
      " 32  sensor_32       220252 non-null  float64\n",
      " 33  sensor_33       220304 non-null  float64\n",
      " 34  sensor_34       220304 non-null  float64\n",
      " 35  sensor_35       220304 non-null  float64\n",
      " 36  sensor_36       220304 non-null  float64\n",
      " 37  sensor_37       220304 non-null  float64\n",
      " 38  sensor_38       220293 non-null  float64\n",
      " 39  sensor_39       220293 non-null  float64\n",
      " 40  sensor_40       220293 non-null  float64\n",
      " 41  sensor_41       220293 non-null  float64\n",
      " 42  sensor_42       220293 non-null  float64\n",
      " 43  sensor_43       220293 non-null  float64\n",
      " 44  sensor_44       220293 non-null  float64\n",
      " 45  sensor_45       220293 non-null  float64\n",
      " 46  sensor_46       220293 non-null  float64\n",
      " 47  sensor_47       220293 non-null  float64\n",
      " 48  sensor_48       220293 non-null  float64\n",
      " 49  sensor_49       220293 non-null  float64\n",
      " 50  sensor_50       143303 non-null  float64\n",
      " 51  sensor_51       204937 non-null  float64\n",
      " 52  machine_status  220320 non-null  object \n",
      "dtypes: float64(52), object(1)\n",
      "memory usage: 90.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# some information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2018-04-01 00:00:00', '2018-04-01 00:01:00',\n",
       "               '2018-04-01 00:02:00', '2018-04-01 00:03:00',\n",
       "               '2018-04-01 00:04:00', '2018-04-01 00:05:00',\n",
       "               '2018-04-01 00:06:00', '2018-04-01 00:07:00',\n",
       "               '2018-04-01 00:08:00', '2018-04-01 00:09:00',\n",
       "               ...\n",
       "               '2018-08-31 23:50:00', '2018-08-31 23:51:00',\n",
       "               '2018-08-31 23:52:00', '2018-08-31 23:53:00',\n",
       "               '2018-08-31 23:54:00', '2018-08-31 23:55:00',\n",
       "               '2018-08-31 23:56:00', '2018-08-31 23:57:00',\n",
       "               '2018-08-31 23:58:00', '2018-08-31 23:59:00'],\n",
       "              dtype='datetime64[ns]', name='timestamp', length=220320, freq=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the index of the dataset\n",
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset starts from April and ends at the end of August."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_divider(df, frequency='M'):\n",
    "\n",
    "    # Make a series of the range in which wants to divide the data\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "\n",
    "    # Slice around 60% of the dataset for training purpose\n",
    "    train_month_number = int(len(dates) * 0.6) - 1\n",
    "    df_train = df.loc[:dates[train_month_number]]\n",
    "\n",
    "    # add test datasets to the divided dataframe dictionary using dic comprehension\n",
    "    divided_df_dict = {f'df_test{number+1}': df.loc[dates[counter]:dates[counter+1]]\n",
    "            for number, counter in enumerate(range(train_month_number, len(dates) - 1, 1))}\n",
    "    \n",
    "    # add training data to the dictionary\n",
    "    divided_df_dict['df_train'] = df_train\n",
    "\n",
    "    return divided_df_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made the above function to divide the dataset into one train data (about 60% of the dataset), and divide rest of the dataset into a number od testing datasets. It can change dynamically based on the date range that is used as the input. Also, I made a version of this function to save the dataframes directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_divider(self, division_range='M'):\n",
    "    \"\"\"\n",
    "    this method divide the dataset into definite number of \n",
    "    training and testing datasets, then save them in a determined path\n",
    "    \"\"\"\n",
    "    # change the directory\n",
    "    output_dir = self.config['output_path']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Make a series of the range in which wants to divide the data\n",
    "    start_date = self.df.index.min()\n",
    "    end_date = self.df.index.max()\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=division_range)\n",
    "\n",
    "    # Slice around 60% of the dataset for training purpose\n",
    "    train_month_number = int(len(dates) * 0.6) - 1\n",
    "    df_train = self.df.loc[:dates[train_month_number]]\n",
    "\n",
    "    # save the train dataset\n",
    "    df_train.to_csv(os.path.join(output_dir, 'df_train.csv'))\n",
    "\n",
    "    # save all the test sets\n",
    "    for number, counter in enumerate(range(train_month_number, len(dates) - 1, 1)):\n",
    "        df_test = self.df.loc[dates[counter]:dates[counter+1]]\n",
    "        df_test.to_csv(os.path.join(output_dir, f'df_test{number+1}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataDivider Class\n",
    "This is one version of the this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another version of this class\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class DataDivider():\n",
    "\n",
    "    def __init__(self, input_path):\n",
    "        self.config = self.config_reader(input_path)\n",
    "        self.df = self.dataframe_maker()\n",
    "        #self.output_dir = self.config['output_path']\n",
    "\n",
    "    def config_reader(self, input_path):\n",
    "        \"\"\"\n",
    "        explanation: This function open config,yaml file \n",
    "        and fetch the gonfigue file information\n",
    "        input: ...\n",
    "        output: configue file\n",
    "        \"\"\"\n",
    "        with open(input_path, \"r\", encoding=\"utf8\") as input_file:\n",
    "            config = yaml.safe_load(input_file)\n",
    "\n",
    "        return config\n",
    "\n",
    "    def dataframe_maker(self):\n",
    "        \"\"\"\n",
    "        make the main dataset based on the configuration file\n",
    "        \"\"\"\n",
    "        # make the dataframe\n",
    "        file_directory, file_name, _ = self.config.values()\n",
    "        os.chdir(file_directory)\n",
    "        df = pd.read_csv(file_name).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "        # change the type of the timestamp column\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "        # set timestamp column as the index\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def dataframe_divider(self, division_range='M'):\n",
    "        \"\"\"\n",
    "        this method divide the dataset into definite number of \n",
    "        training and testing datasets, then save them in a determined path\n",
    "        \"\"\"\n",
    "\n",
    "        # Make a series of the range in which wants to divide the data\n",
    "        start_date = self.df.index.min()\n",
    "        end_date = self.df.index.max()\n",
    "        dates = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "\n",
    "        # Slice around 60% of the dataset for training purpose\n",
    "        train_month_number = int(len(dates) * 0.6) - 1\n",
    "        df_train = self.df.loc[:dates[train_month_number]]\n",
    "\n",
    "        # add test datasets to the divided dataframe dictionary using dic comprehension\n",
    "        divided_df_dict = {f'df_test{number+1}': self.df.loc[dates[counter]:dates[counter+1]]\n",
    "                for number, counter in enumerate(range(train_month_number, len(dates) - 1, 1))}\n",
    "        \n",
    "        # add training data to the dictionary\n",
    "        divided_df_dict['df_train'] = df_train\n",
    "\n",
    "        return divided_df_dict\n",
    "\n",
    "    def dataframe_saver(self, df_dict):\n",
    "        \"\"\"\n",
    "        this method saves the dataframes in a path\n",
    "        \"\"\"\n",
    "        output_dir = self.config['output_path']\n",
    "        for df_name in df_dict.keys():\n",
    "            df_dict[df_name].to_csv(os.path.join(output_dir, f'{df_name}.csv'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # run the DataDivider class\n",
    "    divider = DataDivider('config.yaml')\n",
    "    df_dict = divider.dataframe_divider()\n",
    "    divider.dataframe_saver(df_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage I will make DataManager class first, then I will make Model clas\n",
    "\n",
    "## DataManager\n",
    "The necessary steps are borrowed from my educational notebook. Here, I only used the best preprocessing pipeline.\n",
    "\n",
    "In the first part, I will implement the best preprocessing that I could find.\n",
    "\n",
    "### Preprocesing\n",
    "I will use the following preprocessing pipeline for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop redundant redundant columns\n",
    "df_copy.drop(['sensor_15', 'sensor_50', 'sensor_51'],inplace = True,axis=1)\n",
    "\n",
    "# Fill the null values\n",
    "df_copy.iloc[:,:-1] = df_copy.iloc[:,:-1].fillna(method='bfill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_data(df, smoothing_par, smoothing_method='exponential', standardization=True):\n",
    "    copy_df = df.copy()\n",
    "    float_df = df.iloc[:,:49]\n",
    "\n",
    "    if smoothing_method == 'rolling_mean':\n",
    "        #calculate rolling mean\n",
    "        smoothed_df = float_df.rolling(window=smoothing_par, min_periods=1).mean()\n",
    "    \n",
    "    else:\n",
    "        #calculate exponential smoothing technique\n",
    "        smoothed_dfs = {}\n",
    "\n",
    "        for column in float_df.columns:\n",
    "            model = SimpleExpSmoothing(float_df[column])\n",
    "            smoothed_model = model.fit(smoothing_level=smoothing_par, optimized=True,)\n",
    "            smoothed_dfs[column] = smoothed_model.fittedvalues\n",
    "\n",
    "        smoothed_df = pd.DataFrame(smoothed_dfs)\n",
    "\n",
    "    # standardize the dataframe\n",
    "    if standardization:\n",
    "        smoothed_df = smoothed_df.apply(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "    copy_df.iloc[:, :49] = smoothed_df\n",
    "\n",
    "    return copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "smoothed_df = smoothing_data(df_copy, None, 'exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset ('min_max' method)\n",
    "smoothed_df.iloc[:,:-1] = smoothed_df.iloc[:,:-1].apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>sensor_09</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_41</th>\n",
       "      <th>sensor_42</th>\n",
       "      <th>sensor_43</th>\n",
       "      <th>sensor_44</th>\n",
       "      <th>sensor_45</th>\n",
       "      <th>sensor_46</th>\n",
       "      <th>sensor_47</th>\n",
       "      <th>sensor_48</th>\n",
       "      <th>sensor_49</th>\n",
       "      <th>machine_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:00:00</th>\n",
       "      <td>0.967656</td>\n",
       "      <td>0.679448</td>\n",
       "      <td>0.876598</td>\n",
       "      <td>0.886646</td>\n",
       "      <td>0.904299</td>\n",
       "      <td>0.764272</td>\n",
       "      <td>0.602870</td>\n",
       "      <td>0.683733</td>\n",
       "      <td>0.639780</td>\n",
       "      <td>0.602086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027367</td>\n",
       "      <td>0.045424</td>\n",
       "      <td>0.024159</td>\n",
       "      <td>0.133860</td>\n",
       "      <td>0.071489</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>0.093861</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:01:00</th>\n",
       "      <td>0.967890</td>\n",
       "      <td>0.678843</td>\n",
       "      <td>0.876766</td>\n",
       "      <td>0.885900</td>\n",
       "      <td>0.904084</td>\n",
       "      <td>0.764561</td>\n",
       "      <td>0.602902</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.639681</td>\n",
       "      <td>0.602128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027367</td>\n",
       "      <td>0.045424</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.133860</td>\n",
       "      <td>0.071489</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>0.093861</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:02:00</th>\n",
       "      <td>0.967922</td>\n",
       "      <td>0.678494</td>\n",
       "      <td>0.876894</td>\n",
       "      <td>0.885460</td>\n",
       "      <td>0.903984</td>\n",
       "      <td>0.764594</td>\n",
       "      <td>0.602905</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.639661</td>\n",
       "      <td>0.602138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027367</td>\n",
       "      <td>0.045424</td>\n",
       "      <td>0.024277</td>\n",
       "      <td>0.133860</td>\n",
       "      <td>0.071489</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>0.093861</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:03:00</th>\n",
       "      <td>0.960908</td>\n",
       "      <td>0.682026</td>\n",
       "      <td>0.876991</td>\n",
       "      <td>0.887351</td>\n",
       "      <td>0.907382</td>\n",
       "      <td>0.738774</td>\n",
       "      <td>0.599371</td>\n",
       "      <td>0.679802</td>\n",
       "      <td>0.641305</td>\n",
       "      <td>0.600828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>0.027367</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.023997</td>\n",
       "      <td>0.132881</td>\n",
       "      <td>0.072326</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.242184</td>\n",
       "      <td>0.092546</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:04:00</th>\n",
       "      <td>0.965310</td>\n",
       "      <td>0.680335</td>\n",
       "      <td>0.876612</td>\n",
       "      <td>0.888466</td>\n",
       "      <td>0.900753</td>\n",
       "      <td>0.766351</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.688521</td>\n",
       "      <td>0.644239</td>\n",
       "      <td>0.602696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024120</td>\n",
       "      <td>0.026632</td>\n",
       "      <td>0.042723</td>\n",
       "      <td>0.023586</td>\n",
       "      <td>0.130917</td>\n",
       "      <td>0.072330</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.242165</td>\n",
       "      <td>0.091882</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sensor_00  sensor_01  sensor_02  sensor_03  sensor_04  \\\n",
       "timestamp                                                                    \n",
       "2018-04-01 00:00:00   0.967656   0.679448   0.876598   0.886646   0.904299   \n",
       "2018-04-01 00:01:00   0.967890   0.678843   0.876766   0.885900   0.904084   \n",
       "2018-04-01 00:02:00   0.967922   0.678494   0.876894   0.885460   0.903984   \n",
       "2018-04-01 00:03:00   0.960908   0.682026   0.876991   0.887351   0.907382   \n",
       "2018-04-01 00:04:00   0.965310   0.680335   0.876612   0.888466   0.900753   \n",
       "\n",
       "                     sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n",
       "timestamp                                                                    \n",
       "2018-04-01 00:00:00   0.764272   0.602870   0.683733   0.639780   0.602086   \n",
       "2018-04-01 00:01:00   0.764561   0.602902   0.683734   0.639681   0.602128   \n",
       "2018-04-01 00:02:00   0.764594   0.602905   0.683734   0.639661   0.602138   \n",
       "2018-04-01 00:03:00   0.738774   0.599371   0.679802   0.641305   0.600828   \n",
       "2018-04-01 00:04:00   0.766351   0.598740   0.688521   0.644239   0.602696   \n",
       "\n",
       "                     ...  sensor_41  sensor_42  sensor_43  sensor_44  \\\n",
       "timestamp            ...                                               \n",
       "2018-04-01 00:00:00  ...   0.025424   0.027367   0.045424   0.024159   \n",
       "2018-04-01 00:01:00  ...   0.025424   0.027367   0.045424   0.024242   \n",
       "2018-04-01 00:02:00  ...   0.025424   0.027367   0.045424   0.024277   \n",
       "2018-04-01 00:03:00  ...   0.024127   0.027367   0.044750   0.023997   \n",
       "2018-04-01 00:04:00  ...   0.024120   0.026632   0.042723   0.023586   \n",
       "\n",
       "                     sensor_45  sensor_46  sensor_47  sensor_48  sensor_49  \\\n",
       "timestamp                                                                    \n",
       "2018-04-01 00:00:00   0.133860   0.071489   0.039794   0.245949   0.093861   \n",
       "2018-04-01 00:01:00   0.133860   0.071489   0.039794   0.245949   0.093861   \n",
       "2018-04-01 00:02:00   0.133860   0.071489   0.039794   0.245949   0.093861   \n",
       "2018-04-01 00:03:00   0.132881   0.072326   0.039794   0.242184   0.092546   \n",
       "2018-04-01 00:04:00   0.130917   0.072330   0.039794   0.242165   0.091882   \n",
       "\n",
       "                     machine_status  \n",
       "timestamp                            \n",
       "2018-04-01 00:00:00          NORMAL  \n",
       "2018-04-01 00:01:00          NORMAL  \n",
       "2018-04-01 00:02:00          NORMAL  \n",
       "2018-04-01 00:03:00          NORMAL  \n",
       "2018-04-01 00:04:00          NORMAL  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **feature engineering** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_one_hot(df):\n",
    "    \n",
    "    # make one hot encoder\n",
    "    status_series = df.machine_status\n",
    "    one_hot = pd.get_dummies(status_series)\n",
    "    one_hot = one_hot.astype(int)\n",
    "    return one_hot\n",
    "\n",
    "def feature_engineering(df):\n",
    "\n",
    "    float_df = df.iloc[:,:49]\n",
    "\n",
    "    # make one hot encoder\n",
    "    one_hot = making_one_hot(df)\n",
    "    \n",
    "    # extract feature importance scores\n",
    "    selector = SelectKBest(score_func=chi2)\n",
    "    selector.fit(float_df, one_hot['NORMAL'])\n",
    "    \n",
    "    return selector\n",
    "\n",
    "def score_sorter(df, fit):\n",
    "    rank_dict = {}\n",
    "    names = df.columns\n",
    "\n",
    "    # make a dictionary of scores\n",
    "    for number, _ in enumerate(fit.scores_):\n",
    "        rank_dict[names[number]] = fit.scores_[number]\n",
    "\n",
    "    # sort the scores\n",
    "    rank_dict = dict(sorted(rank_dict.items(), key=lambda item: -1 * item[1]))\n",
    "\n",
    "    return rank_dict\n",
    "\n",
    "def cropping_df(df, fit, slice_number):\n",
    "\n",
    "    # make the rank dictionary\n",
    "    rank_dict = score_sorter(df, fit)\n",
    "\n",
    "    # make name list\n",
    "    name_list = list(rank_dict.keys())[:slice_number]\n",
    "    \n",
    "\n",
    "    # crop the dataframe\n",
    "    selected_df = df.iloc[:,:49].loc[:,name_list]\n",
    "    selected_df['machine_status'] = df['machine_status']\n",
    "\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>sensor_09</th>\n",
       "      <th>sensor_48</th>\n",
       "      <th>machine_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:00:00</th>\n",
       "      <td>0.967656</td>\n",
       "      <td>0.792070</td>\n",
       "      <td>0.691492</td>\n",
       "      <td>0.904299</td>\n",
       "      <td>0.489296</td>\n",
       "      <td>0.602870</td>\n",
       "      <td>0.876598</td>\n",
       "      <td>0.683733</td>\n",
       "      <td>0.054064</td>\n",
       "      <td>0.639780</td>\n",
       "      <td>0.602086</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:01:00</th>\n",
       "      <td>0.967890</td>\n",
       "      <td>0.792070</td>\n",
       "      <td>0.691492</td>\n",
       "      <td>0.904084</td>\n",
       "      <td>0.489296</td>\n",
       "      <td>0.602902</td>\n",
       "      <td>0.876766</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.054064</td>\n",
       "      <td>0.639681</td>\n",
       "      <td>0.602128</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:02:00</th>\n",
       "      <td>0.967922</td>\n",
       "      <td>0.792070</td>\n",
       "      <td>0.691492</td>\n",
       "      <td>0.903984</td>\n",
       "      <td>0.489296</td>\n",
       "      <td>0.602905</td>\n",
       "      <td>0.876894</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.054064</td>\n",
       "      <td>0.639661</td>\n",
       "      <td>0.602138</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:03:00</th>\n",
       "      <td>0.960908</td>\n",
       "      <td>0.802954</td>\n",
       "      <td>0.712980</td>\n",
       "      <td>0.907382</td>\n",
       "      <td>0.497670</td>\n",
       "      <td>0.599371</td>\n",
       "      <td>0.876991</td>\n",
       "      <td>0.679802</td>\n",
       "      <td>0.054869</td>\n",
       "      <td>0.641305</td>\n",
       "      <td>0.600828</td>\n",
       "      <td>0.242184</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:04:00</th>\n",
       "      <td>0.965310</td>\n",
       "      <td>0.810934</td>\n",
       "      <td>0.703873</td>\n",
       "      <td>0.900753</td>\n",
       "      <td>0.507023</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.876612</td>\n",
       "      <td>0.688521</td>\n",
       "      <td>0.051105</td>\n",
       "      <td>0.644239</td>\n",
       "      <td>0.602696</td>\n",
       "      <td>0.242165</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sensor_00  sensor_11  sensor_12  sensor_04  sensor_10  \\\n",
       "timestamp                                                                    \n",
       "2018-04-01 00:00:00   0.967656   0.792070   0.691492   0.904299   0.489296   \n",
       "2018-04-01 00:01:00   0.967890   0.792070   0.691492   0.904084   0.489296   \n",
       "2018-04-01 00:02:00   0.967922   0.792070   0.691492   0.903984   0.489296   \n",
       "2018-04-01 00:03:00   0.960908   0.802954   0.712980   0.907382   0.497670   \n",
       "2018-04-01 00:04:00   0.965310   0.810934   0.703873   0.900753   0.507023   \n",
       "\n",
       "                     sensor_06  sensor_02  sensor_07  sensor_13  sensor_08  \\\n",
       "timestamp                                                                    \n",
       "2018-04-01 00:00:00   0.602870   0.876598   0.683733   0.054064   0.639780   \n",
       "2018-04-01 00:01:00   0.602902   0.876766   0.683734   0.054064   0.639681   \n",
       "2018-04-01 00:02:00   0.602905   0.876894   0.683734   0.054064   0.639661   \n",
       "2018-04-01 00:03:00   0.599371   0.876991   0.679802   0.054869   0.641305   \n",
       "2018-04-01 00:04:00   0.598740   0.876612   0.688521   0.051105   0.644239   \n",
       "\n",
       "                     sensor_09  sensor_48 machine_status  \n",
       "timestamp                                                 \n",
       "2018-04-01 00:00:00   0.602086   0.245949         NORMAL  \n",
       "2018-04-01 00:01:00   0.602128   0.245949         NORMAL  \n",
       "2018-04-01 00:02:00   0.602138   0.245949         NORMAL  \n",
       "2018-04-01 00:03:00   0.600828   0.242184         NORMAL  \n",
       "2018-04-01 00:04:00   0.602696   0.242165         NORMAL  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run SelectKBest\n",
    "selector = feature_engineering(smoothed_df)\n",
    "\n",
    "selected_df = cropping_df(smoothed_df, selector, 12)\n",
    "selected_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above preprocessing steps the following set of classes are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "\n",
    "# statistical libraries\n",
    "from statsmodels.tsa.api import  SimpleExpSmoothing\n",
    "\n",
    "# Feature Engineering module\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "class DataManager():\n",
    "\n",
    "    def __init__(self, df, fill_method, smoothing_par, smoothing_method, norm_name):\n",
    "        self.df = df\n",
    "\n",
    "        # Preprocessing parameters\n",
    "        self.fill_method = fill_method\n",
    "        self.smoothing_par = smoothing_par\n",
    "        self.smoothing_method = smoothing_method\n",
    "\n",
    "        # Normalization parameters\n",
    "        self.norm_name = norm_name\n",
    "\n",
    "    def dataframe_manager(self):\n",
    "        \"\"\"\n",
    "        This function implements all the preprocessing steps on the dataframe\n",
    "        and returns the transformed dataset to the main function.\n",
    "        \"\"\"\n",
    "        # Preprocessing part\n",
    "        preprocessing_obj = Prprocessing(self.df, self.fill_method, self.smoothing_par,\n",
    "                                         self.smoothing_method)\n",
    "        self.df = preprocessing_obj.datframe_pruner()\n",
    "        self.df = preprocessing_obj.dataframe_smoother()\n",
    "\n",
    "        # Normalization part\n",
    "        normalization_obj = Normalization(self.df, self.norm_name)\n",
    "        self.df = normalization_obj.method_selector()\n",
    "\n",
    "        # Feature engineering\n",
    "        feature_engineering_obj = FeatureEngineering(self.df)\n",
    "        self.df = feature_engineering_obj.dataframe_cropper(12)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "class Prprocessing():\n",
    "\n",
    "    def __init__(self, df, fill_method, smoothing_par, smoothing_method):\n",
    "        self.df = df \n",
    "        self.fill_method = fill_method\n",
    "        self.smoothing_par = smoothing_par\n",
    "        self.smoothing_method = smoothing_method\n",
    "\n",
    "    def datframe_pruner(self):\n",
    "        \"\"\"\n",
    "        This function prune (drop) the necessary data columns and fill the Nan values\n",
    "        \"\"\"\n",
    "        # Make a copy of the dataset\n",
    "        df_processed = self.df.copy()\n",
    "\n",
    "        # Drop redundant columns\n",
    "        df_processed.drop(['sensor_15', 'sensor_50', 'sensor_51'],inplace = True,axis=1)\n",
    "\n",
    "        # Fill the null values\n",
    "        df_processed.iloc[:,:-1] = df_processed.iloc[:,:-1].fillna(method=self.fill_method)\n",
    "\n",
    "        return df_processed\n",
    "\n",
    "    def dataframe_smoother(self):\n",
    "        \"\"\"\n",
    "        This function implements a smoothing technique on the dataset\n",
    "        \"\"\"\n",
    "        df_copy = self.df.copy()\n",
    "\n",
    "        # Slice the floating part\n",
    "        float_df = self.df.iloc[:,:49]\n",
    "\n",
    "        if self.smoothing_method == 'rolling_mean':\n",
    "            #calculate rolling mean\n",
    "            smoothed_df = float_df.rolling(window=self.smoothing_par, min_periods=1).mean()\n",
    "\n",
    "        else:\n",
    "            #calculate exponential smoothing technique\n",
    "            smoothed_dfs = {}\n",
    "\n",
    "            for column in float_df.columns:\n",
    "                model = SimpleExpSmoothing(float_df[column])\n",
    "                smoothed_model = model.fit(smoothing_level=self.smoothing_par, optimized=True,)\n",
    "                smoothed_dfs[column] = smoothed_model.fittedvalues\n",
    "\n",
    "            smoothed_df = pd.DataFrame(smoothed_dfs)\n",
    "\n",
    "            df_copy.iloc[:, :49] = smoothed_df\n",
    "\n",
    "        return df_copy\n",
    "\n",
    "class Normalization():\n",
    "\n",
    "    def __init__(self, df, norm_name):\n",
    "        self.df = df\n",
    "        self.norm_name = norm_name\n",
    "\n",
    "    def max_normalizer(self): \n",
    "        \"\"\"\n",
    "        This function returns 'max' normalization\n",
    "        \"\"\"\n",
    "        df_normalized = self.df.apply(lambda x: x / x.abs().max())\n",
    "        return df_normalized\n",
    "\n",
    "    def min_max_normalizer(self):\n",
    "        \"\"\"\n",
    "        This function returns 'min_max normalization\n",
    "        \"\"\"\n",
    "        df_normalized = self.df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "        return df_normalized\n",
    "\n",
    "    def z_score_normalizer(self):\n",
    "        \"\"\"\n",
    "        This function returns z-score normalization\n",
    "        \"\"\"\n",
    "        df_normalized = self.df.apply(lambda x: (x - x.mean()) / x.std())\n",
    "        return df_normalized\n",
    "\n",
    "    def method_selector(self):\n",
    "        \"\"\"\n",
    "        input: 1- method_name: this string determines the method's type.\n",
    "        explanation: This function chooses the right endmember extraction method based\n",
    "                     on the input name. And, return the endmembers based on the method.\n",
    "        output: 1- an endmember method\n",
    "        \"\"\"\n",
    "        # make the function name\n",
    "        method_name = f'{self.norm_name}_normalizer'\n",
    "        normalization_method = getattr(self, method_name)\n",
    "\n",
    "        # return the spectrum\n",
    "        return normalization_method()\n",
    "\n",
    "class FeatureEngineering():\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.selector = self.feature_engineering()\n",
    "    \n",
    "    def making_one_hot(self):\n",
    "        \"\"\"\n",
    "        This function make one_hot encoder from label column\n",
    "        \"\"\"\n",
    "        # make one hot encoder\n",
    "        status_series = self.df.machine_status\n",
    "        one_hot = pd.get_dummies(status_series)\n",
    "        one_hot = one_hot.astype(int)\n",
    "        return one_hot\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        \"\"\"\n",
    "        This function implements SelectKBest model on the dataset\n",
    "        \"\"\"\n",
    "        float_df = self.df.iloc[:,:49]\n",
    "\n",
    "        # make one hot encoder\n",
    "        one_hot = self.making_one_hot()\n",
    "\n",
    "        # extract feature importance scores\n",
    "        selector = SelectKBest(score_func=chi2)\n",
    "        selector.fit(float_df, one_hot['NORMAL'])\n",
    "\n",
    "        return selector\n",
    "\n",
    "    def score_sorter(self):\n",
    "        \"\"\"\n",
    "        This function make a sorted dictionary of the columns and their\n",
    "        scores in an ascending manner\n",
    "        \"\"\"\n",
    "        rank_dict = {}\n",
    "        names = self.df.columns\n",
    "\n",
    "        # make a dictionary of scores\n",
    "        for number,_ in enumerate(self.selector.scores_):\n",
    "            rank_dict[names[number]] = self.selector.scores_[number]\n",
    "\n",
    "        # sort the scores\n",
    "        rank_dict = dict(sorted(rank_dict.items(), key=lambda item: -1 * item[1]))\n",
    "\n",
    "        return rank_dict\n",
    "\n",
    "    def dataframe_cropper(self, slice_number):\n",
    "        \"\"\"\n",
    "        This function crops the columns in the dataset with the highest\n",
    "        scores in another dataset\n",
    "        \"\"\"\n",
    "        # make the rank dictionary\n",
    "        rank_dict = self.score_sorter()\n",
    "\n",
    "        # make name list\n",
    "        name_list = list(rank_dict.keys())[:slice_number]\n",
    "\n",
    "        # crop the dataframe\n",
    "        selected_df = self.df.iloc[:,:49].loc[:,name_list]\n",
    "        selected_df['machine_status'] = self.df['machine_status']\n",
    "\n",
    "        return selected_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is the time for training the model.\n",
    "\n",
    "# Model Training\n",
    "While working on Assignment 3 for Machine Learning, I discovered that the 'Isolation Forest' model is exceptionally well-suited for this particular dataset (one can have a look at the mentioned assignment for tones of detail about all the methods :)). As a result, I've decided to employ this model for our task. To optimize its performance, I'll utilize GridSearchCV to fine-tune its parameters. For evaluating the model's effectiveness, I'll consider metrics such as 'f1', 'ROC-AUC', and accuracy.\n",
    "\n",
    "However, I'm prioritizing accuracy as the primary assessment metric for this method. The reason behind this choice is that the model's key strength lies in its ability to accurately predict system malfunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_grid_search(estimator_dict, scoring_list, cv_number, refit_method, data_dict):\n",
    "\n",
    "    final_dict = {}\n",
    "\n",
    "    for name in estimator_dict.keys():\n",
    "        # Create the GridSearchCV object\n",
    "        grid_search = GridSearchCV(estimator=estimator_dict[name][0], param_grid=estimator_dict[name][1],\n",
    "                                    scoring=scoring_list, cv=cv_number, refit=refit_method)\n",
    "        \n",
    "        # Fit the the best model to the data\n",
    "        grid_search.fit(data_dict['x'], data_dict['y'])\n",
    "\n",
    "        # Save the best estimator for each model\n",
    "        final_dict[name] = {'best_model': grid_search.best_estimator_,\n",
    "                            'best_parameters': grid_search.best_params_,\n",
    "                            'best_score': grid_search.best_score_}\n",
    "\n",
    "    # order the dictionary based on the magnitude of the scores\n",
    "    final_dict = dict(sorted(final_dict.items(), key=lambda item: -1 * item[1]['best_score']))\n",
    "     \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(df):\n",
    "\n",
    "    # Convert label column into 1 and -1, ro make it comparable to the model \n",
    "    y_train = [1 if element=='NORMAL' else -1 for element in df.iloc[:,-1]]\n",
    "    X = np.array(df.iloc[:,:-1])\n",
    "\n",
    "    # Make data dictionary\n",
    "    data_dict =  {'x': X, \n",
    "                'y': y_train}\n",
    "\n",
    "    # Find the outlier fraction in the dataset\n",
    "    normal_rows = df[df['machine_status']=='NORMAL']\n",
    "    outliers_fraction = 1 - (len(normal_rows)/(len(df)))\n",
    "\n",
    "    # make parameters dictionaries for Isolation Forest\n",
    "    if_param= {\n",
    "        'n_estimators': [100],\n",
    "        'max_samples': [1.0],\n",
    "        'contamination': [outliers_fraction],\n",
    "        'max_features': [1.0],\n",
    "        'n_jobs': [-1]\n",
    "        }\n",
    "\n",
    "    # Scoring list\n",
    "    scoring_list = {'roc_auc', 'accuracy', 'f1'}\n",
    "\n",
    "    # Make estimator dictionary\n",
    "    estimator_dict={'IsolationForest': [IsolationForest(), if_param]}\n",
    "\n",
    "    final_dict = multiple_grid_search(estimator_dict, scoring_list, 5, 'accuracy', data_dict)\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"c:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 399, in _score\n",
      "    return self._sign * self._score_func(y, y_pred, **self._kwargs)\n",
      "  File \"c:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\", line 572, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"c:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"c:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\", line 339, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'IsolationForest': {'best_model': IsolationForest(contamination=0.06574074074074077, max_samples=1.0, n_jobs=-1),\n",
       "  'best_parameters': {'contamination': 0.06574074074074077,\n",
       "   'max_features': 1.0,\n",
       "   'max_samples': 1.0,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1},\n",
       "  'best_score': 0.8916984386347131}}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict = model_trainer(selected_df)\n",
    "final_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
